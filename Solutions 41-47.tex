\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,latexsym}
%\usepackage{amsmath,amssymb,latexsym,pstricks}

\parindent 0in
\addtolength{\textwidth}{52mm}
\addtolength{\oddsidemargin}{-28mm}
\addtolength{\evensidemargin}{-26mm}
\addtolength{\topmargin}{-20mm}
\addtolength{\textheight}{40mm}

\newcommand{\ph}{\phantom}
\newcommand{\ds}{\displaystyle}
\newcommand{\tvs}{\textvisiblespace}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\vc}[1]{\begin{pmatrix}#1\end{pmatrix}}
\newcommand{\tmat}[1]{{\left(\begin{array}{rrr}#1\end{array}\right)}}
\newcommand{\qmat}[1]{{\left(\begin{array}{rrrr}#1\end{array}\right)}}
\newcommand{\pmat}[1]{{\left(\begin{array}{rrrrr}#1\end{array}\right)}}
\newcommand{\proof}{{\sc Proof.}\quad}
\newcommand{\qed}{\quad$\square$}

\newcommand{\moveup}{\begin{picture}(0,0)(0,0)\end{picture}\vspace*{-8.15mm}}
\newcommand{\upabit}{\begin{picture}(0,0)(0,0)\end{picture}\vspace*{-5mm}}

\newcommand{\ra}[1]{\xrightarrow{#1}}
\newcommand{\dra}[2]{\begin{array}{c}\xrightarrow{\text{\scriptsize$#1$}}\\[-2mm]{\text{\scriptsize$#2$}}\end{array}}


\date{}
\author{}
\title{\sc Solutions to MATH3411 Problems 41--47}


\begin{document} \maketitle

\vspace*{-10mm}

\noindent{\bf 41.}
$H(S) = -\frac{2}{3}\log_2\frac{1}{3}-\frac{3}{9}\log_2\frac{1}{9}
      =  \frac{2}{3}\log_2 3 + \frac{6}{9}\log_2 3
      =  \frac{4}{3}\log_2 3
      \approx 2.113$

\bigskip\noindent{\bf 42.}
\begin{itemize}
  \item[{Q29:}]
    \begin{itemize}
      \item[{\bf a)}] $H(S) = -\frac{1}{2}\log_2\frac{1}{2} -\frac{1}{3}\log_2\frac{1}{3} -\frac{1}{6}\log_2\frac{1}{6} \approx 1.459$
        \\The average length $L = 1.5 > H(S)$ - but pretty close.
      \item[{\bf b)}] $H(S) = -\frac{1}{3}\log_2\frac{1}{3} -\frac{1}{4}\log_2\frac{1}{4} -\frac{1}{5}\log_2\frac{1}{5} -\frac{1}{6}\log_2\frac{1}{6} -\frac{1}{20}\log_2\frac{1}{20} \approx 2.140$
        \\The average length $L = 2.217 > H(S)$ - but not far off.
      \item[{\bf c)}] $H(S) = -\frac{1}{2}\log_2\frac{1}{2} -\frac{1}{4}\log_2\frac{1}{4} -\frac{1}{8}\log_2\frac{1}{8} -\frac{1}{16}\log_2\frac{1}{16} -\frac{1}{16}\log_2\frac{1}{16} \approx 1.875$
        \\The average length $L = 1.875 = H(S)$ - exactly the same!
      \item[{\bf d)}] $H(S) = -\frac{27}{40}\log_2\frac{27}{40} -\frac{9}{40}\log_2\frac{9}{40} -\frac{3}{40}\log_2\frac{3}{40} -\frac{1}{40}\log_2\frac{1}{40} \approx 1.280$
        \\The average length $L = 1.425 > H(S)$ - not too far off.
    \end{itemize}
  \item[{Q33:}] \moveup
        \begin{align*} H(S) &=           - 0.22\log_4 0.22 - 0.2 \log_4 0.2  - 0.18\log_4 0.18 - 0.15\log_4 0.15\\
                            &\phantom{=} - 0.10\log_4 0.10 - 0.08\log_4 0.08 - 0.05\log_4 0.05 - 0.02\log_2 0.02\\
                            &\approx 1.377\end{align*}
        The average length $L =  1.47$ is greater than $H(S) \approx 1.377$ - but it's pretty close.
  \item[{Q35:}] $H(S^1) = -\frac{2}{3}\log_2\frac{2}{3} -\frac{1}{3}\log_2\frac{1}{3} \approx 0.918$
        \\$H(S^2) = 2H(S^1) \approx 1.837$
        \\$H(S^3) = 3H(S^1) \approx 2.755$
        \\The corresponding average lengths are $1$, 1.889 and 2.815, respectively.
        \\These are all greater than the corresponding entropies - but not by a lot.
  \item[{Q39:}] $H(S) = -0.4\log_{10}0.4 -0.3\log_{10}0.3 - 0.2\log_{10}0.2 -0.1\log_{10}0.1 \approx 0.5558$ digits/symbol.
        \\The 5-symbol message $s_2s_1s_3s_1\,\bullet$ was encoded as $0.493$, so $\frac{3}{5} = 0.6$ digits per symbol were used,
        \\which is more than 0.5558 but not by a lot.
\end{itemize}

\bigskip\noindent{\bf 43.}
Measured in information per bits,
the entropy of the experiment is
\[
  H(S) = H_2(S)
       = -\frac{2}{3}\log_2\frac{1}{3} - \frac{2}{9}\log_2\frac{1}{9} - \frac{3}{27}\log_2\frac{1}{27}
       \approx 2.2894
\]
According to p.78 of the course notes, the average codeword length per symbol of the extension $S^n$ for increasing $n$
converges to $H(S)$, so the price for encoded long messages per bit will approximately be $2.2894\times\$2.00\approx \$4.56$.

Now,
measured in information per ternary units,
the entropy of the experiment is
\[
    H_3(S)
  = -\frac{2}{3}\log_3\frac{1}{3} - \frac{2}{9}\log_3\frac{1}{9} - \frac{3}{27}\log_3\frac{1}{27}
  =  \frac{13}{9}
\]
The price for encoded long messages per ternary unit will approximately be $\frac{13}{9}\times\$3.25\approx \$4.69$.

We see that for sufficiently long messages, the binary encoding is (slightly) cheaper.


\newpage\noindent{\bf 44.}
\begin{itemize}
  \item[{Q29:}]\moveup
      \[\hspace*{-10mm}\textbf{a)}\:
        \begin{array}{cccr}
                    &               & \text{SF} & \text{SF\;\:}\\[-1mm]
           p_i      & \frac{1}{p_i} & \ell_i &\!\text{code}\!\\[1mm]\hline\\[-5mm]
        \frac{1}{2} &     2         &   1    &   0\\
        \frac{1}{3} &     3         &   2    &  10\\
        \frac{1}{6} &     6         &   3    & 110\end{array}\qquad
        \textbf{b)}\:
        \begin{array}{cccr}
                    &               & \text{SF} & \text{SF\;\:}\\[-1mm]
           p_i      & \frac{1}{p_i} & \ell_i    &\!\text{code}\!\\[1mm]\hline\\[-5mm]
        \frac{1}{3} &     3         &   2       &    00\\
        \frac{1}{4} &     4         &   2       &    01\\
        \frac{1}{5} &     5         &   3       &   100\\
        \frac{1}{6} &     6         &   3       &   101\\
        \frac{1}{20}&    20         &   5       & 11000\end{array}\qquad
        \textbf{c)}\:
        \begin{array}{cccr}
                    &               & \text{SF} & \text{SF\;\:}\\[-1mm]
           p_i      & \frac{1}{p_i} & \ell_i    &\!\text{code}\!\\[1mm]\hline\\[-5mm]
        \frac{1}{2} &     2         &    1      &     0\\
        \frac{1}{4} &     4         &    2      &    10\\
        \frac{1}{8} &     8         &    3      &   110\\
        \frac{1}{16}&    16         &    4      &  1110\\
        \frac{1}{32}&    32         &    5      & 11110\end{array}\qquad
        \textbf{d)}\:
        \begin{array}{cccr}
                      &               & \text{SF} & \text{SF\;\:}\\[-1mm]
           p_i        & \frac{1}{p_i} & \ell_i    &\!\text{code}\!\\[1mm]\hline\\[-5mm]
        \frac{27}{40} & \frac{40}{27} &  1        &      0\\
        \frac{9}{40} & \frac{40}{9}  &  3        &    100\\
        \frac{3}{40} & \frac{40}{3}  &  4        &   1010\\
        \frac{1}{40} &       40      &  6        & 101100\end{array}
      \]
  \item[{Q33:}] {\bf NB}: Here, we use radix 4:
       \[\begin{array}{cccr}
                      &               & \text{SF} & \text{SF\;\:}\\[-1mm]
           p_i        & \frac{1}{p_i} & \ell_i    &\!\text{code}\!\\[1mm]\hline\\[-5mm]
           0.22       &     4.5       &  2        &   00\\
           0.20       &     5         &  2        &   01\\
           0.18       &     5.6       &  2        &   02\\
           0.15       &     6.7       &  2        &   03\\
           0.10       &     10        &  2        &   10\\
           0.08       &     12.5      &  2        &   11\\
           0.05       &     20        &  3        &  100\\
           0.02       &     50        &  3        &  101\end{array}\]
  \item[{Q35:}]
       \[\begin{array}{cccr}
           S        &               & \text{SF} &  \text{SF\;\:}\\[-1mm]
           p_i      & \frac{1}{p_i} & \ell_i    &\!\text{code}\!\\[1mm]\hline\\[-5mm]
        \frac{2}{3} & \frac{3}{2}   &   1       &   0\\
        \frac{1}{3} &     3         &   2       &  10\end{array}
        \qquad\quad
        \begin{array}{cccr}
           S^{(2)}  &               & \text{SF} &  \text{SF\;\:}\\[-1mm]
           p_i      & \frac{1}{p_i} & \ell_i    &\!\text{code}\!\\[1mm]\hline\\[-5mm]
        \frac{4}{9} & \frac{9}{4}   &   2       &    00\\
        \frac{2}{9} & \frac{9}{2}   &   3       &   100\\
        \frac{2}{9} & \frac{9}{2}   &   3       &   101\\
        \frac{1}{9} &     9         &   4       &  1100\end{array}
        \qquad\quad
        \begin{array}{cccr}
           S^{(3)}   &               & \text{SF} &  \text{SF\;\:}\\[-1mm]
           p_i       & \frac{1}{p_i} & \ell_i    &\!\text{code}\!\\[1mm]\hline\\[-5mm]
        \frac{8}{27} & \frac{27}{8}  &   2       &    00\\
        \frac{4}{27} & \frac{27}{4}  &   3       &   010\\
        \frac{4}{27} & \frac{27}{4}  &   3       &   011\\
        \frac{4}{27} & \frac{27}{4}  &   3       &   100\\
        \frac{2}{27} & \frac{27}{2}  &   4       &  1010\\
        \frac{2}{27} & \frac{27}{2}  &   4       &  1011\\
        \frac{2}{27} & \frac{27}{2}  &   4       &  1100\\
        \frac{1}{27} &     27        &   5       & 11010\end{array}
        \]
\end{itemize}

\bigskip\noindent{\bf 45.}\upabit
\begin{itemize}
   \item[{\bf a)}] $H_2(S) = -\frac{1}{3} \log_2\frac{1}{3}  -\frac{1}{4} \log_2\frac{1}{4} -\frac{1}{6}\log_2\frac{1}{6}
                             -\frac{3}{20}\log_2\frac{3}{20} -\frac{1}{10}\log_2\frac{1}{10}
                    \approx 2.20$
   \item[{\bf b)}] {\bf NB}: Here, we use radix 3:
          $\begin{array}{cccr}
                    &               & \text{SF} &  \text{SF\;\:}\\[-1mm]
           p_i      & \frac{1}{p_i} & \ell_i    &\!\text{code}\!\\[1mm]\hline\\[-5mm]
        \frac{1}{3} &      3        &   1       &    0\\
        \frac{1}{4} &      4        &   2       &   10\\
        \frac{1}{6} &      6        &   2       &   11\\
        \frac{3}{20}& \frac{20}{3}  &   2       &   12\\
        \frac{1}{10}&     10        &   3       &  200\end{array}$
     \\The average length $L = \frac{1}{3}\times 2 + \frac{1}{4}\times 2 + \frac{1}{6}\times 2 + \frac{3}{20}\times 2 + \frac{1}{10}\times 3 = \frac{53}{30} \approx 1.77$.
     \\(This is bigger than the entropy found in part a) - but that was the {\bf binary} entropy, not the {\bf ternary} entropy,
        so there is no contradiction here.)
   \item[{\bf c)}] The smallest two probabilities for $S^{(4)}$ are $p_{624} = \frac{3}{20\cdot10^3}$ and $p_{625} = \frac{1}{10^4}$;
     \\their inverses are
     $\frac{1}{p_{624}} = \frac{2}{3}\times 10^4 \approx 6667$ and
     $\frac{1}{p_{625}} = 10^4 = 10000$,
     \\so $\ell_{624} = 13$ and $\ell_{625} = 14$ \: ($2^{13} = 8192$ and $2^{14} = 16384$)
\end{itemize}

\bigskip\noindent{\bf 46.}
$H(p) = -p\log_2 p - (1-p)\log_2(1-p)$.
\\Note that $H(0) = -0 - 1\log_2 1 = 0$
  and  that $H(1) = -1\log_2 1 - 0 = 0$.
\\Also, for $p\in (0,1)$, $-\log_2 p\geq 0$ and $-\log_2 (1-p)\geq 0$,
  so $H(p) \geq 0$.
\\Hence, $H(p)$ is non-negative on the interval $[0,1]$.
\\Now note that $H'(p) = -\log_2 p + \log_2(1-p)$.
\\For $p<\frac{1}{2}$, we see that $-\log_2 p > 1$ whereas $\log_2(1-p)>-1$, so $H'(p) > 0$;
\\similarly for $p>\frac{1}{2}$, we see that $-\log_2 p < 1$ whereas $\log_2(1-p)<-1$, $H'(p) < 0$.
\\Therefore, $H(p)$ is concave down.
\\We also see that the maximum of $H(p)$ is at $p = \frac{1}{2}$ and equals $H(\frac{1}{2}) = 1$.

\bigskip\noindent{\bf 47.}\moveup\\[-2mm]
\begin{itemize}
  \item[{Q36:}] Here, the equilibrium vector is $\frac{1}{11}\vc{3\\4\\4}$.
    \begin{align*}
      H(S|s_1) &= - \frac{1}{ 3}\log_2 \frac{1}{ 3} - \frac{1}{ 3}\log_2 \frac{1}{ 3} - \frac{1}{ 3}\log_2 \frac{1}{ 3} = \log_2 3 \approx 1.58\\
      H(S|s_2) &= - \frac{1}{ 4}\log_2 \frac{1}{ 4} - \frac{1}{ 2}\log_2 \frac{1}{ 2} - \frac{1}{ 4}\log_2 \frac{1}{ 4} = \frac{3}{2} = 1.5\\
      H(S|s_3) &= H(S|s_2) = 1.5\\
      H_M     &=   \frac{3}{11}H(S|s_1) + \frac{4}{11}H(S|s_2) + \frac{4}{11}H(S|s_3) \approx 1.52\\
      H_E      &= - \frac{3}{11}\log_2 \frac{3}{11} - \frac{4}{11}\log_2 \frac{4}{11} - \frac{4}{11}\log_2 \frac{4}{11} \approx 1.57\\
    \end{align*}
  \item[{Q37:}] Here, the equilibrium vector is $\frac{1}{17}\vc{6\\7\\4}$.
    \begin{align*}
      H(S|s_1) &= - 0.7\log_2 0.7 - 0.2\log_2 0.2 - 0.1\log_2 0.1 \approx 1.16\\
      H(S|s_2) &= - 0.2\log_2 0.2 - 0.6\log_2 0.6 - 0.2\log_2 0.2 \approx 1.37\\
      H(S|s_2) &= - 0.1\log_2 0.1 - 0.4\log_2 0.4 - 0.5\log_2 0.5 \approx 1.36\\
      H_M     &=   \frac{6}{17}H(S|s_1) + \frac{7}{17}H(S|s_2) + \frac{4}{17}H(S|s_3) \approx 1.29\\
      H_E      &= - \frac{6}{17}\log_2 \frac{6}{17} - \frac{7}{17}\log_2 \frac{7}{17} - \frac{4}{17}\log_2 \frac{4}{17} \approx 1.55\\
    \end{align*}
\end{itemize}

\end{document}

\bigskip\noindent{\bf 48.} Define the events
  \begin{align*}
      A &= \text{``Passes the course"}\\
      C &= \text{``Owns a car"}       \\
      H &= \text{``Lives at home"}
    \end{align*}
  We are given the following probabilities:
  \[
    \begin{array}{lllclll}
      P(A)             &=& 0.75  & \qquad& P(H|C)           &=& 1.00\\
      P(C|A)           &=& 0.10  &       & P(H|C^c\cap A^c) &=& 0.40\\
      P(C|A^c)         &=& 0.50  &       & P(H|C^c\cap A)   &=& 0.40
    \end{array}
  \]
%It follows that $P(H|C^c) = 0.4$ and
We can illustrate these probabilities as follows:
\begin{center}
\begin{pspicture}(0,-0.5)(13,4)
%\psgrid[subgriddiv=1,griddots=10,gridlabels=10pt] (-1,0)(13,4)
\psline[arrowsize=5pt]{->}(0,0)(4.8,0)
\psline[arrowsize=5pt]{->}(0,3)(4.8,0.2)
\psline[arrowsize=5pt]{->}(0,0)(4.8,2.8)
\psline[arrowsize=5pt]{->}(0,3)(4.8,3)
\uput[l](0,3){Pass}\rput[r](-1,3){0.75}
\uput[l](0,0){Fail}\rput[r](-1,0){0.25}
\uput[r](5,3){Own Car}
\uput[r](5,0){No Car}
\psdot[dotsize=5pt](0,0)
\psdot[dotsize=5pt](0,3)
\psdot[dotsize=5pt](5,3)
\psdot[dotsize=5pt](5,0)
\uput[u](2.5,3){$0.1$}
\uput[d](2.5,0){$0.5$}
\uput[u](1.5,.80){$0.5$}
\uput[u](2,2){$0.9$}
\psline[arrowsize=5pt]{->}(7,0)(11.8,0)
%\psline[arrowsize=5pt]{->}(7,3)(11.8,0.2)
%\psline[arrowsize=5pt]{->}(7,0)(11.8,2.8)
\psline[arrowsize=5pt]{->}(7,3)(11.8,3)
\uput[r](12,3){At home}
\uput[r](12,0){Away}
\psdot[dotsize=5pt](7,0)
\psdot[dotsize=5pt](7,3)
\psdot[dotsize=5pt](12,3)
\psdot[dotsize=5pt](12,0)
\uput[u](9.5,3){$1$}
\uput[d](9.5,0){$0.6$}
%\uput[u](9.5,1.5){$0.4$}
\end{pspicture}
\end{center}
From the diagram, we see that
\[
    P(C)
%  = P((C\cap A)\cup (C\cap A^c)
%  = P(C\cap A) + P(C\cap A^c)
%  = P(C|A)P(A) + P(C|A^c)P(A^c)
  = 0.75\times 0.1 + 0.25\times 0.5
  = 0.2
\]
In particular
\[
  P(C\cap A)   = P(C|A  )P(A)   = 0.75\times 0.1  = 0.075\\qquad\text{and}\qquad
  P(C\cap A^c) = P(C|A^c)P(A^c) = 0.50\times 0.25 = 0.125
\]
Therefore,
\[
  P(H\cap C) = P(H\cap C\cap A) + P(H\cap C\cap A^c)
             = P(H|C\cap A)P(C\cap A) + P(H|C\cap A^c)P(C\cap A^c)
             = P(

\]


We see from this that $P(z_1\,|\, x_1) = 0.1\times1+0.9\times0.4=0.46$ and
$P(z_1\,|\, x_2) = 0.5\times 1 + 0.5\times 0.4 = 0.7$. Hence (or from the diagram)
$P(z_2\,|\, x_1) = 0.54$ and $P(z_2\,|\, x_2) = 0.3$.

We also have $P(y_1)=0.1\times 0.75 + 0.5\times 0.25 = 0.2$, so $P(y_2)=0.8$ and
thus $P(z_1) = 0.2\times1 + 0.8\times0.4=0.52$ and $P(z_2)=0.48$.

With these we can calculate all the required information using the usual function
$H(x)=-x\log_2(x)-(1-x)\log_2(1-x)$.

\begin{enumerate}
  \item This is asking to show that $P(z_k\,|\,x_i\,\cap\,y_j) =
P(z_k\,|\,y_j)$. But since all car owners stay at home $P(z_1\,|\,y_1)$ is
independent of $x_i$, and $P(z_2\,|\,y_1)=0$ independently of $x_i$. The question
explicitly states that $P(z_1\,|\,y_2)$ is the same for each value of $x_i$, and it
follows that $P(z_2\,|\,y_2)$ is the same for each value of $x_i$ as well.

Formally, (and more generally) if $x_1$ and $x_2$ are complementary events and
$P(A\,|\,B\,\cap\,x_1)=P(A\,|\,B\,\cap\, x_2)$ then
$P(A\,|\,B\,\cap\, x_i)=P(A\,|\,B)$ for either $i$. To prove this, we have
\begin{align*}
  P(A\,\cap\, B)
  &=P(A\,\cap\, B\,\cap\, x_1)+P(A\,\cap\, B\,\cap\, x_2)\\
  &=P(A\,|\,B\,\cap\, x_1)P(B\,\cap\, x_1)+P(A\,|\,B\,\cap\, x_2)P(B\,\cap\,
x_2)\\
  &=P(A\,|\,B\,\cap\, x_1)\bigl(P(B\,\cap\, x_1)+P(B\,\cap\, x_2)\bigr)\\
  &=P(A\,|\,B\,\cap\, x_1)P(B)
\end{align*}
and so
$P(A\,|\,B\,\cap\, x_1)=\dfrac{P(A\,\cap\, B)}{P(B)}=P(A\,|\,B)$
as claimed, and the result for $x_2$ is similar. Applying this to $A=z_k$, $B=y_j$
gives our result.

\item We need the information in $X$ given $Y$:
\begin{align*}
  I(X\,,\, Y ) &= H(Y) - H(Y\,|\, X) = H(0.2) -\left(0.75H(0.1) +
0.25H(0.5)\right)\\ &\approx 0.12\ \text{bits}.
\end{align*}
\item This is asking for the information in $X$ given $Z$:
\begin{align*}
 I(X\,,\, Z ) &= H(Z) - H(Z\,|\, X) = H(0.52) -\left(0.75H(0.46) +
0.25H(0.7)\right)\\ &\approx 0.03\ \text{bits}.
\end{align*}
\item The information in the first digit is $H(X) = H(0.75)\approx 0.811$ bits.

The (new or extra) information in the second digit is $H(Y\,|\,X) = 0.75H(0.1) +
0.25H(0.5))\approx 0.602$ bits.

The (new or extra) information in the third digit is
$H(Z\,|\, X\,\cap\,Y)=H(Z\,|\,Y)$, since
$P(z_k\,|\,x_i\,\cap\,y_j) = P(z_k\,|\,y_j).$
But this is
$0.2H(1)+0.8H(0.4)\approx0.777$.

\end{enumerate}



\end{document}

